/**
 * CLI entry point for the data pipeline scripts.
 */
import { Command } from "commander";
import { aggregateToCsv } from "./aggregate";
import { loadChunksToCSV } from "./loadChunks";
import { loadPagesToCSV } from "./loadPages";
import {
	AUTHORS_QUERY,
	AUTHORSHIP_QUERY,
	NOTABLES_QUERY,
	WORKS_QUERY,
} from "./sparql";
import {
	cleanAllGroups,
	getLatestAuthors,
	getLatestWorks,
	getMostRecentFilename,
	getNewPath,
} from "./storage";

/**
 * Validates that a user input number is a positive integer.
 */
function validatePosInt(value: string, name: string): number {
	const num = parseInt(value, 10);
	if (Number.isNaN(num)) {
		throw new Error(`${name} must be a valid number`);
	}
	if (num < 0) {
		throw new Error(`${name} must be non-negative`);
	}
	if (!Number.isInteger(num)) {
		throw new Error(`${name} must be an integer`);
	}
	return num;
}

/**
 * Queries the Wikidata SPARQL endpoint for authorship data and saves it to
 * a CSV file.
 *
 * @param chunkSize - The number of results to fetch per page. Lower this if
 * 	you experience 504 errors from Wikidata.
 */
async function authorships(chunkSize: number): Promise<void> {
	const filename = await getNewPath("authorships");
	await loadPagesToCSV(filename, AUTHORSHIP_QUERY, chunkSize);
}

/**
 * Queries the Wikidata SPARQL endpoint for notable authorship data and
 * saves it to a CSV file.
 *
 * @param chunkSize - The number of results to fetch per page. Lower this if
 * 	you experience 504 errors from Wikidata.
 */
async function notables(chunkSize: number): Promise<void> {
	const filename = await getNewPath("notables");
	await loadPagesToCSV(filename, NOTABLES_QUERY, chunkSize);
}

/**
 * Queries the Wikidata SPARQL endpoint for author data and saves it to
 * a CSV file.
 *
 * @param initialOffset - The initial offset for pagination.
 * @param chunkSize - The number of results to fetch per chunk. Lower this if
 * 	you experience 504 errors from Wikidata.
 */
async function authors(
	initialOffset: number,
	chunkSize: number,
): Promise<void> {
	const filename = await getNewPath("authors");
	const authors = await getLatestAuthors();
	await loadChunksToCSV(
		filename,
		AUTHORS_QUERY,
		authors,
		initialOffset,
		chunkSize,
	);
}

/**
 * Queries the Wikidata SPARQL endpoint for work data and saves it to
 * a CSV file.
 *
 * @param initialOffset - The initial offset for pagination.
 * @param chunkSize - The number of results to fetch per chunk. Lower this if
 * 	you experience 504 errors from Wikidata.
 */
async function works(initialOffset: number, chunkSize: number): Promise<void> {
	const filename = await getNewPath("works");
	const works = await getLatestWorks();
	await loadChunksToCSV(filename, WORKS_QUERY, works, initialOffset, chunkSize);
}

/**
 * Aggregates the data from all previously generated CSV files and a duckdb
 * database with pageviews into a single JSON file.
 *
 * This JSON file serves as the database for the application.
 *
 * @param db_path - Path to the DuckDB database generated by `pvduck`.
 */
async function aggregate(db_path: string): Promise<void> {
	const [filename, authors_path, works_path, authorships_path, notables_path] =
		await Promise.all([
			getNewPath("aggregate", "json"),
			getMostRecentFilename("authors"),
			getMostRecentFilename("works"),
			getMostRecentFilename("authorships"),
			getMostRecentFilename("notables"),
		]);

	if (!authors_path || !works_path || !authorships_path || !notables_path) {
		console.error("Missing required CSV files for aggregation.");
		return;
	}

	await aggregateToCsv(
		filename,
		authors_path,
		works_path,
		authorships_path,
		notables_path,
		db_path,
	);
}

// Entry point for CLI
const program = new Command();

program
	.name("bookbear-cli")
	.description("CLI for the Book Bear data pipeline scripts")
	.version("1.0.0");

program
	.command("authorships")
	.description("Create a CSV file with all author-work pairs")
	.option("-c, --chunk-size <size>", "Number of results per page", "100000")
	.action(async (options: { chunkSize: string }) => {
		const chunkSize = validatePosInt(options.chunkSize, "Chunk size");
		await authorships(chunkSize);
	});

program
	.command("notables")
	.description("Create a CSV file with all notable author-work pairs")
	.option("-c, --chunk-size <size>", "Number of results per page", "100000")
	.action(async (options: { chunkSize: string }) => {
		const chunkSize = validatePosInt(options.chunkSize, "Chunk size");
		await notables(chunkSize);
	});

program
	.command("authors")
	.description("Create a CSV file with detailed author data")
	.option("-o, --offset <number>", "Initial offset for pagination", "0")
	.option("-c, --chunk-size <size>", "Number of results per chunk", "10000")
	.action(async (options: { offset: string; chunkSize: string }) => {
		const offset = validatePosInt(options.offset, "Offset");
		const chunkSize = validatePosInt(options.chunkSize, "Chunk size");
		await authors(offset, chunkSize);
	});

program
	.command("works")
	.description("Create a CSV file with detailed works data")
	.option("-o, --offset <number>", "Initial offset for pagination", "0")
	.option("-c, --chunk-size <size>", "Number of results per chunk", "10000")
	.action(async (options: { offset: string; chunkSize: string }) => {
		const offset = validatePosInt(options.offset, "Offset");
		const chunkSize = validatePosInt(options.chunkSize, "Chunk size");
		await works(offset, chunkSize);
	});

program
	.command("aggregate <db_path>")
	.description("Aggregate the data in the database")
	.action(async (db_path: string) => {
		await aggregate(db_path);
	});

program
	.command("clean")
	.description("Delete all old data files, keeping only the most recent ones")
	.action(async () => {
		const results = await cleanAllGroups();

		let totalDeleted = 0;
		for (const [group, count] of results.entries()) {
			if (count > 0) {
				console.log(`- Deleted ${count} old ${group} file(s)`);
				totalDeleted += count;
			}
		}

		if (totalDeleted === 0) {
			console.log("No old files to delete");
		} else {
			console.log(`Cleanup complete: ${totalDeleted} file(s) deleted`);
		}
	});

program.parse();
