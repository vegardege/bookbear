/**
 * CLI entry point for the data pipeline scripts.
 */
import { aggregateToCsv } from "./aggregate.js";
import { loadChunksToCSV } from "./loadChunks.js";
import { loadPagesToCSV } from "./loadPages.js";
import {
  AUTHORS_QUERY,
  AUTHORSHIP_QUERY,
  NOTABLES_QUERY,
  WORKS_QUERY,
} from "./sparql.js";
import {
  getNewPath,
  getMostRecentFilename,
  getLatestAuthors,
  getLatestWorks,
} from "./storage.js";

/**
 * Queries the Wikidata SPARQL endpoint for authorship data and saves it to
 * a CSV file.
 */
async function authorships(): Promise<void> {
  const filename = getNewPath("authorships");
  await loadPagesToCSV(filename, AUTHORSHIP_QUERY, 100_000);
}

/**
 * Queries the Wikidata SPARQL endpoint for notable authorship data and
 * saves it to a CSV file.
 */
async function notables(): Promise<void> {
  const filename = getNewPath("notables");
  await loadPagesToCSV(filename, NOTABLES_QUERY, 100_000);
}

/**
 * Queries the Wikidata SPARQL endpoint for author data and saves it to
 * a CSV file.
 *
 * @param initialOffset - The initial offset for pagination.
 */
async function authors(initialOffset: number): Promise<void> {
  const filename = getNewPath("authors");
  const authors = getLatestAuthors();
  await loadChunksToCSV(
    filename,
    AUTHORS_QUERY,
    authors,
    initialOffset,
    10_000
  );
}

/**
 * Queries the Wikidata SPARQL endpoint for work data and saves it to
 * a CSV file.
 *
 * @param initialOffset - The initial offset for pagination.
 */
async function works(initialOffset: number): Promise<void> {
  const filename = getNewPath("works");
  const works = getLatestWorks();
  await loadChunksToCSV(filename, WORKS_QUERY, works, initialOffset, 10_000);
}

/**
 * Aggregates the data from all previously generated CSV files and a duckdb
 * database with pageviews into a single JSON file.
 *
 * This JSON file serves as the database for the application.
 *
 * @param db_path - Path to the DuckDB database generated by `pvduck`.
 */
async function aggregate(db_path: string): Promise<void> {
  const filename = getNewPath("aggregate", "json");
  const authors_path = getMostRecentFilename("authors");
  const works_path = getMostRecentFilename("works");
  const authorships_path = getMostRecentFilename("authorships");
  const notables_path = getMostRecentFilename("notables");

  if (!authors_path || !works_path || !authorships_path || !notables_path) {
    console.error("Missing required CSV files for aggregation.");
    return;
  }

  aggregateToCsv(
    filename,
    authors_path,
    works_path,
    authorships_path,
    notables_path,
    db_path
  );
}

// Entry point for CLI
const args = process.argv.slice(2);

switch (args[0]) {
  case "authorships":
    await authorships();
    break;
  case "notables":
    await notables();
    break;
  case "authors":
    await authors(args[1] ? parseInt(args[1]) : 0);
    break;
  case "works":
    await works(args[1] ? parseInt(args[1]) : 0);
    break;
  case "aggregate":
    await aggregate(args[1]);
    break;
  default:
    console.error(`Unknown command: ${args[0]}`);
    console.error("Usage: node run script <command>");
    console.error("Commands:");
    console.error(
      "  authorships - Create a CSV file with all author-work pairs" +
        "  notables - Create a CSV file with all notable author-work pairs" +
        "  authors - Create a CSV file with detailed author data" +
        "  works - Create a CSV file with detailed works data" +
        "  aggregate /path/to/db.duckdb - Aggregate the data in the database"
    );
    break;
}
