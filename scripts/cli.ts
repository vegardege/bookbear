/**
 * CLI entry point for the data pipeline scripts.
 */
import { aggregateToCsv } from "./aggregate.js";
import { loadChunksToCSV } from "./loadChunks.js";
import { loadPagesToCSV } from "./loadPages.js";
import {
	AUTHORS_QUERY,
	AUTHORSHIP_QUERY,
	NOTABLES_QUERY,
	WORKS_QUERY,
} from "./sparql.js";
import {
	getLatestAuthors,
	getLatestWorks,
	getMostRecentFilename,
	getNewPath,
} from "./storage.js";

/**
 * Queries the Wikidata SPARQL endpoint for authorship data and saves it to
 * a CSV file.
 */
async function authorships(): Promise<void> {
	const filename = await getNewPath("authorships");
	await loadPagesToCSV(filename, AUTHORSHIP_QUERY, 100_000);
}

/**
 * Queries the Wikidata SPARQL endpoint for notable authorship data and
 * saves it to a CSV file.
 */
async function notables(): Promise<void> {
	const filename = await getNewPath("notables");
	await loadPagesToCSV(filename, NOTABLES_QUERY, 100_000);
}

/**
 * Queries the Wikidata SPARQL endpoint for author data and saves it to
 * a CSV file.
 *
 * @param initialOffset - The initial offset for pagination.
 */
async function authors(initialOffset: number): Promise<void> {
	const filename = await getNewPath("authors");
	const authors = await getLatestAuthors();
	await loadChunksToCSV(
		filename,
		AUTHORS_QUERY,
		authors,
		initialOffset,
		10_000,
	);
}

/**
 * Queries the Wikidata SPARQL endpoint for work data and saves it to
 * a CSV file.
 *
 * @param initialOffset - The initial offset for pagination.
 */
async function works(initialOffset: number): Promise<void> {
	const filename = await getNewPath("works");
	const works = await getLatestWorks();
	await loadChunksToCSV(filename, WORKS_QUERY, works, initialOffset, 10_000);
}

/**
 * Aggregates the data from all previously generated CSV files and a duckdb
 * database with pageviews into a single JSON file.
 *
 * This JSON file serves as the database for the application.
 *
 * @param db_path - Path to the DuckDB database generated by `pvduck`.
 */
async function aggregate(db_path: string): Promise<void> {
	const [filename, authors_path, works_path, authorships_path, notables_path] =
		await Promise.all([
			getNewPath("aggregate", "json"),
			getMostRecentFilename("authors"),
			getMostRecentFilename("works"),
			getMostRecentFilename("authorships"),
			getMostRecentFilename("notables"),
		]);

	if (!authors_path || !works_path || !authorships_path || !notables_path) {
		console.error("Missing required CSV files for aggregation.");
		return;
	}

	await aggregateToCsv(
		filename,
		authors_path,
		works_path,
		authorships_path,
		notables_path,
		db_path,
	);
}

// Entry point for CLI
const args = process.argv.slice(2);

switch (args[0]) {
	case "authorships":
		await authorships();
		break;
	case "notables":
		await notables();
		break;
	case "authors":
		await authors(args[1] ? parseInt(args[1]) : 0);
		break;
	case "works":
		await works(args[1] ? parseInt(args[1]) : 0);
		break;
	case "aggregate":
		await aggregate(args[1]);
		break;
	default:
		console.error(`Unknown command: ${args[0]}`);
		console.error("Usage: node run script <command>");
		console.error("Commands:");
		console.error(
			"  authorships - Create a CSV file with all author-work pairs" +
				"  notables - Create a CSV file with all notable author-work pairs" +
				"  authors - Create a CSV file with detailed author data" +
				"  works - Create a CSV file with detailed works data" +
				"  aggregate /path/to/db.duckdb - Aggregate the data in the database",
		);
		break;
}
